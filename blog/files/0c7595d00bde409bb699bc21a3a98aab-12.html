<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
	<head>

		<meta charset="utf-8" />

		<!-- User defined head content such as meta tags and encoding options -->
		
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-132321370-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-132321370-1');
</script><link rel="mask-icon" href="../logo4.svg" color="#000000" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta name="description" content="Blogs about DS/DL/ML/Big data and my life" />
		<meta name="robots" content="index, follow" />
		<meta name="generator" content="RapidWeaver" />
		<link rel="icon" href="http://lipengyuan1994.github.io/favicon.ico" type="image/x-icon" />
		<link rel="shortcut icon" href="http://lipengyuan1994.github.io/favicon.ico" type="image/x-icon" />
		<link rel="apple-touch-icon" href="http://lipengyuan1994.github.io/apple-touch-icon.png" />
		

		<!-- User defined head content -->
		<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-132321370-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-132321370-1');
</script>

<script data-ad-client="ca-pub-5942763238935595" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>




		<!-- Meta tags -->
	  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />

		<title>Coursera Natural Language Processing in TensorFlow  | Blogs about DS/DL/ML/Big data and my life</title>

		<!-- Load some fonts from Google's Font service -->
		<link href='https://fonts.googleapis.com/css?family=Open Sans:400,300,800|Noto Serif|Arvo' rel='stylesheet' type='text/css'>

		<!-- CSS stylesheets reset -->
	  <link rel="stylesheet" type="text/css" media="all" href="../../rw_common/themes/tesla/consolidated-6.css?rwcache=596864305" />
		

		<!-- CSS for the Foundation framework's CSS that handles the responsive columnized layout -->
	  

	  <!-- Main Stylesheet -->
		

	  <!-- Loads Font Awesome v4.0.3 CSS from CDN -->
	  <link href="https://netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
	
		<!-- RapidWeaver Color Picker Stylesheet -->
		  

	  <!-- Theme specific media queries -->
		

		<!-- Base RapidWeaver javascript -->
		<script type="text/javascript" src="../../rw_common/themes/tesla/javascript.js?rwcache=596864305"></script>

		<!-- jQuery 1.8 is included in the theme internally -->
	  <script src="../../rw_common/themes/tesla/js/jquery.min.js?rwcache=596864305"></script>

	  <!-- Theme specific javascript, along with jQuery Easing and a few other elements -->
	  <script src="../../rw_common/themes/tesla/js/elixir.js?rwcache=596864305"></script>

		<!-- Style variations -->
		

		<!-- User defined styles -->
		

		<!-- User defined javascript -->
		

		<!-- Plugin injected code -->
		<link rel="alternate" type="application/rss+xml" title="Patrick's blog" href="http://feeds.feedburner.com/tingWoBB" />
<script type="text/javascript" async src="https://lipengyuan1994.github.io/blog/files/meta.js"></script>


	
	</head>
	
	<!-- This page was created with RapidWeaver from Realmac Software. http://www.realmacsoftware.com -->

	<body>

			<header role="banner">

				<!-- Site Logo -->
				<div id="logo" data-0="opacity: 1;" data-top-bottom="opacity: 0;" data-anchor-target="#logo">
			  	  
				</div>
				
				<div id="title_wrapper">
						<!-- Site Title -->
						<h1 id="site_title" data-0="opacity: 1; top:0px;" data-600="opacity: 0; top: 80px;" data-anchor-target="#site_title">
							Welcome to my BLOG
						</h1>

						<!-- Site Slogan -->
						<h2 id="site_slogan" data-0="opacity: 1; top:0px;" data-600="opacity: 0; top: 80px;" data-anchor-target="#site_slogan">
							Constant dripping wears away a stone.
						</h2>

						<!-- Scroll down button -->
						<div id="scroll_down_button" data-0="opacity: 1; top:0px;" data-400="opacity: 0; top: 100px;" data-anchor-target="#scroll_down_button">
							<i class="fa fa-angle-down"></i>
						</div>
				</div>
				
				<!-- Top level navigation -->
				<div id="navigation_bar">
					<div class="row site_width">
						<div class="large-12 columns">
							<nav id="top_navigation"><ul><li><a href="../../" rel="">HOME</a></li><li><a href="../" rel="" id="current">BLOG</a></li><li><a href="../../page-4/" rel="">ABOUT ME</a></li><li><a href="../../blog-4/" rel="">NEWS</a></li><li><a href="../../page-3/" rel="" class="parent">More About Me</a></li></ul></nav>
						</div>
					</div>
				</div>

			</header>

		<!-- Sub-navigation -->
		<div id="sub_navigation_bar">
			<div class="row site_width">
				<div class="large-12 columns">
					<nav id="sub_navigation"><ul></ul></nav>
				</div>
			</div>
		</div>

		<!-- Mobile Navigation -->
		<div id="mobile_navigation_toggle">
			<i id="mobile_navigation_toggle_icon" class="fa fa-bars"></i>
		</div>
		<nav id="mobile_navigation">
			<ul><li><a href="../../" rel="">HOME</a></li><li><a href="../" rel="" id="current">BLOG</a></li><li><a href="../../page-4/" rel="">ABOUT ME</a></li><li><a href="../../blog-4/" rel="">NEWS</a></li><li><a href="../../page-3/" rel="" class="parent">More About Me</a></li></ul>
		</nav>

		<!-- Main Content area and sidebar -->
		<div class="row site_width" id="container">
			<section id="content"class="large-8 columns">
					
	<div class="blog-archive-entries-wrapper">
		<div id="unique-entry-id-12" class="blog-entry"><h1 class="blog-entry-title">Coursera Natural Language Processing in TensorFlow </h1><div class="blog-entry-date">Dec/07/2019 11:31 <span class="blog-entry-category"><a href="category-mooc.html">Mooc</a></span><span class="blog-entry-category"> | <a href="category-data-science.html">Data Science</a></span></div><div class="blog-entry-body"><span style="font:17px HelveticaNeue; color:#262626;">Week 1 Quiz Graded Quiz<br /><br /></span><span style="font:14px HelveticaNeue; color:#262626;"><br />Week 1 Quiz<br />Graded Quiz<br />Due Dec 16, 2:59 AM EST<br />Week 1 Quiz<br />TOTAL POINTS 8<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">1.Question 1<br /></span><span style="font:14px HelveticaNeue; color:#262626;">What is the name of the object used to tokenize sentences?<br />CharacterTokenizer<br />TextTokenizer<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Tokenizer</span><span style="font:14px HelveticaNeue; color:#262626;"><br />WordTokenizer<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">2.Question 2<br /></span><span style="font:14px HelveticaNeue; color:#262626;">What is the name of the method used to tokenize a list of sentences?<br />fit_to_text(sentences)<br />tokenize(sentences)<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">fit_on_texts(sentences)</span><span style="font:14px HelveticaNeue; color:#262626;"><br />tokenize_on_text(sentences)<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">3.Question 3<br /></span><span style="font:14px HelveticaNeue; color:#262626;">Once you have the corpus tokenized, what&rsquo;s the method used to encode a list of sentences to use those tokens?<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">texts_to_sequences(sentences)</span><span style="font:14px HelveticaNeue; color:#262626;"><br />text_to_sequences(sentences)<br />text_to_tokens(sentences)<br />texts_to_tokens(sentences)<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">4.Question 4<br /></span><span style="font:14px HelveticaNeue; color:#262626;">When initializing the tokenizer, how to you specify a token to use for unknown words?<br />unknown_word=<br />unknown_token=<br />out_of_vocab=<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">oov_token=</span><span style="font:14px HelveticaNeue; color:#262626;"><br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">5.Question 5<br /></span><span style="font:14px HelveticaNeue; color:#262626;">If you don&rsquo;t use a token for out of vocabulary words, what happens at encoding?<br />The word is replaced by the most common token<br />The word isn&rsquo;t encoded, and the sequencing ends<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">The word isn&rsquo;t encoded, and is skipped in the sequence</span><span style="font:14px HelveticaNeue; color:#262626;"><br />The word isn&rsquo;t encoded, and is replaced by a zero in the sequence<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">6.Question 6<br /></span><span style="font:14px HelveticaNeue; color:#262626;">If you have a number of sequences of different lengths, how do you ensure that they are understood when fed into a neural network?<br />Process them on the input layer of the Neural Netword using the pad_sequences property<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Use the pad_sequences object from the tensorflow.keras.preprocessing.sequence namespace</span><span style="font:14px HelveticaNeue; color:#262626;"><br />Specify the input layer of the Neural Network to expect different sizes with dynamic_length<br />Make sure that they are all the same length using the pad_sequences method of the tokenizer<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">7.Question 7<br /></span><span style="font:14px HelveticaNeue; color:#262626;">If you have a number of sequences of different length, and call pad_sequences on them, what&rsquo;s the default result?<br />They&rsquo;ll get cropped to the length of the shortest sequence<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">They&rsquo;ll get padded to the length of the longest sequence by adding zeros to the beginning of shorter ones</span><span style="font:14px HelveticaNeue; color:#262626;"><br />They&rsquo;ll get padded to the length of the longest sequence by adding zeros to the end of shorter ones<br />Nothing, they&rsquo;ll remain unchanged<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">8.Question 8<br /></span><span style="font:14px HelveticaNeue; color:#262626;">When padding sequences, if you want the padding to be at the end of the sequence, how do you do it?<br />Call the padding method of the pad_sequences object, passing it &lsquo;after&rsquo;<br />Call the padding method of the pad_sequences object, passing it &lsquo;post&rsquo;<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Pass padding=&rsquo;post&rsquo; to pad_sequences when initializing it</span><span style="font:14px HelveticaNeue; color:#262626;"><br />Pass padding=&rsquo;after&rsquo; to pad_sequences when initializing it<br />1 point<br /><br /><br /></span><span style="font:17px HelveticaNeue; color:#262626;">Week 2 Quiz Graded Quiz<br /><br /></span><span style="font:28px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">QUIZ<br />Week 2 Quiz<br /></span><span style="font:14px HelveticaNeue; color:#262626;">People are making progress<br />883 learners have recently completed this assignment<br />Submit your assignment<br />DUEDec 23, 2:59 AM EST<br />ATTEMPTS3 every 8 hours<br />Receive grade<br />TO PASS80% or higher Grade<br />&mdash;<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">1.Question 1<br /></span><span style="font:14px HelveticaNeue; color:#262626;">What is the name of the TensorFlow library containing common data that you can use to train and test neural networks?<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">TensorFlow Datasets</span><span style="font:14px HelveticaNeue; color:#262626;"><br />There is no library of common data sets, you have to use your own<br />TensorFlow Data<br />TensorFlow Data Libraries<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">2.Question 2<br /></span><span style="font:14px HelveticaNeue; color:#262626;">How many reviews are there in the IMDB dataset and how are they split?<br />60,000 records, 50/50 train/test split<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">50,000 records, 50/50 train/test split</span><span style="font:14px HelveticaNeue; color:#262626;"><br />50,000 records, 80/20 train/test split<br />60,000 records, 80/20 train/test split<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">3.Question 3<br /></span><span style="font:14px HelveticaNeue; color:#262626;">How are the labels for the IMDB dataset encoded?<br />Reviews encoded as a boolean true/false<br />Reviews encoded as a number 1-5<br />Reviews encoded as a number 1-10<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Reviews encoded as a number 0-1</span><span style="font:14px HelveticaNeue; color:#262626;"><br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">4.Question 4<br /></span><span style="font:14px HelveticaNeue; color:#262626;">What is the purpose of the embedding dimension?<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">It is the number of dimensions for the vector representing the word encoding</span><span style="font:14px HelveticaNeue; color:#262626;"><br />It is the number of dimensions required to encode every word in the corpus<br />It is the number of words to encode in the embedding<br />It is the number of letters in the word, denoting the size of the encoding<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">5.Question 5<br /></span><span style="font:14px HelveticaNeue; color:#262626;">When tokenizing a corpus, what does the num_words=n parameter do?<br />It specifies the maximum number of words to be tokenized, and picks the first &lsquo;n&rsquo; words that were tokenized<br />It specifies the maximum number of words to be tokenized, and stops tokenizing when it reaches n<br />It errors out if there are more than n distinct words in the corpus<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">It specifies the maximum number of words to be tokenized, and picks the most common &lsquo;n&rsquo; words</span><span style="font:14px HelveticaNeue; color:#262626;"><br /></span><span style="font:12px Menlo-Regular; color:#262626;">num_words: the maximum number of words to keep, based on word frequency.</span><span style="font:14px HelveticaNeue; color:#262626;"><br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">6.Question 6<br /></span><span style="font:14px HelveticaNeue; color:#262626;">To use word embeddings in TensorFlow, in a sequential layer, what is the name of the class?<br />tf.keras.layers.WordEmbedding<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">tf.keras.layers.Embedding</span><span style="font:14px HelveticaNeue; color:#262626;"><br />tf.keras.layers.Word2Vector<br />tf.keras.layers.Embed<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">7.Question 7<br /></span><span style="font:14px HelveticaNeue; color:#262626;">IMDB Reviews are either positive or negative. What type of loss function should be used in this scenario?<br />Adam<br />Binary Gradient descent<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Binary crossentropy</span><span style="font:14px HelveticaNeue; color:#262626;"><br />Categorical crossentropy<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">8.Question 8<br /></span><span style="font:14px HelveticaNeue; color:#262626;">When using IMDB Sub Words dataset, our results in classification were poor. Why?<br />The sub words make no sense, so can&rsquo;t be classified<br />Our neural network didn&rsquo;t have enough layers<br />We didn&rsquo;t train long enough<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Sequence becomes much more important when dealing with subwords, but we&rsquo;re ignoring word positions</span><span style="font:14px HelveticaNeue; color:#262626;"><br />1 point<br /><br /><br /><br /></span><span style="font:17px HelveticaNeue; color:#262626;">Week 3 Quiz Graded Quiz<br /></span><span style="font:14px HelveticaNeue; color:#262626;">Natural Language Processing in TensorFlow<br />Week 3<br /></span><span style="font:28px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Week 3 Quiz<br />Sequence models<br /></span><span style="font:14px HelveticaNeue; color:#262626;">People are making progress<br />848 learners have recently completed this assignment<br />Submit your assignment<br />DUEDec 30, 2:59 AM EST<br />ATTEMPTS3 every 8 hours<br />Receive grade<br />TO PASS80% or higher<br />Grade<br />&mdash;<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">1.Question 1<br /></span><span style="font:14px HelveticaNeue; color:#262626;">Why does sequence make a large difference when determining semantics of language?<br />Because the order in which words appear dictate their meaning<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Because the order in which words appear dictate their impact on the meaning of the sentence</span><span style="font:14px HelveticaNeue; color:#262626;"><br />Because the order of words doesn&rsquo;t matter<br />It doesn&rsquo;t<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">2.Question 2<br /></span><span style="font:14px HelveticaNeue; color:#262626;">How do Recurrent Neural Networks help you understand the impact of sequence on meaning?<br />They don&rsquo;t<br />They look at the whole sentence at a time<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">They carry meaning from one cell to the next</span><span style="font:14px HelveticaNeue; color:#262626;"><br />They shuffle the words evenly<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">3.Question 3<br /></span><span style="font:14px HelveticaNeue; color:#262626;">How does an LSTM help understand meaning when words that qualify each other aren&rsquo;t necessarily beside each other in a sentence?<br />They shuffle the words randomly<br />They load all words into a cell state<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Values from earlier words can be carried to later ones via a cell state</span><span style="font:14px HelveticaNeue; color:#262626;"><br />They don&rsquo;t<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">4.Question 4<br /></span><span style="font:14px HelveticaNeue; color:#262626;">What keras layer type allows LSTMs to look forward and backward in a sentence?<br />Unilateral<br />Bothdirection<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Bidirectional</span><span style="font:14px HelveticaNeue; color:#262626;"><br />Bilateral<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">5.Question 5<br /></span><span style="font:14px HelveticaNeue; color:#262626;">What&rsquo;s the output shape of a bidirectional LSTM layer with 64 units?<br />(128,1)<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">(None, 128)</span><span style="font:14px HelveticaNeue; color:#262626;"><br />(None, 64)<br />(128,None)<br />1 point<br /></span><span style="font:21px HelveticaNeue-BoldItalic; font-weight:bold; color:#262626;font-weight:bold; "><em>6.Question 6</em></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; "><br /></span><span style="font:14px HelveticaNeue; color:#262626;">When stacking LSTMs, how do you instruct an LSTM to feed the next one in the sequence?<br />Do nothing, TensorFlow handles this automatically<br />Ensure that return_sequences is set to True on all units<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Ensure that return_sequences is set to True only on units that feed to another LSTM</span><span style="font:14px HelveticaNeue; color:#262626;"><br />Ensure that they have the same number of units<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">7.Question 7<br /></span><span style="font:14px HelveticaNeue; color:#262626;">If a sentence has 120 tokens in it, and a Conv1D with 128 filters with a Kernal size of 5 is passed over it, what&rsquo;s the output shape?<br />(None, 120, 128)<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">(None, 116, 128)</span><span style="font:14px HelveticaNeue; color:#262626;"><br />(None, 120, 124)<br />(None, 116, 124)<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">8.Question 8<br /></span><span style="font:14px HelveticaNeue; color:#262626;">What&rsquo;s the best way to avoid overfitting in NLP datasets?<br />Use LSTMs<br />Use GRUs<br />Use Conv1D<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">None of the above</span><span style="font:14px HelveticaNeue; color:#262626;"><br />1 point<br /><br /></span><span style="font:17px HelveticaNeue; color:#262626;">Week 4 Quiz Graded Quiz<br /><br /></span><span style="font:14px HelveticaNeue; color:#262626;">Week 4 Quiz<br />Graded Quiz<br />Due Jan 6, 2:59 AM EST<br />Week 4 Quiz<br />TOTAL POINTS 8<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">1.Question 1<br /></span><span style="font:14px HelveticaNeue; color:#262626;">What is the name of the method used to tokenize a list of sentences?<br />tokenize_on_text(sentences)<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">fit_on_texts(sentences)</span><span style="font:14px HelveticaNeue; color:#262626;"><br />fit_to_text(sentences)<br />tokenize(sentences)<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">2.Question 2<br /></span><span style="font:14px HelveticaNeue; color:#262626;">If a sentence has 120 tokens in it, and a Conv1D with 128 filters with a Kernal size of 5 is passed over it, what&rsquo;s the output shape?<br />(None, 120, 124)<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">(None, 116, 128)</span><span style="font:14px HelveticaNeue; color:#262626;"><br />(None, 116, 124)<br />(None, 120, 128)<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">3.Question 3<br /></span><span style="font:14px HelveticaNeue; color:#262626;">What is the purpose of the embedding dimension?<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">It is the number of dimensions for the vector representing the word encoding</span><span style="font:14px HelveticaNeue; color:#262626;"><br />It is the number of dimensions required to encode every word in the corpus<br />It is the number of words to encode in the embedding<br />It is the number of letters in the word, denoting the size of the encoding<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">4.Question 4<br /></span><span style="font:14px HelveticaNeue; color:#262626;">IMDB Reviews are either positive or negative. What type of loss function should be used in this scenario?<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Binary crossentropy</span><span style="font:14px HelveticaNeue; color:#262626;"><br />Binary Gradient descent<br />Categorical crossentropy<br />Adam<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">5.Question 5<br /></span><span style="font:14px HelveticaNeue; color:#262626;">If you have a number of sequences of different lengths, how do you ensure that they are understood when fed into a neural network?<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Use the pad_sequences object from the tensorflow.keras.preprocessing.sequence namespace</span><span style="font:14px HelveticaNeue; color:#262626;"><br />Process them on the input layer of the Neural Network using the pad_sequences property<br />Specify the input layer of the Neural Network to expect different sizes with dynamic_length<br />Make sure that they are all the same length using the pad_sequences method of the tokenizer<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">6.Question 6<br /></span><span style="font:14px HelveticaNeue; color:#262626;">When predicting words to generate poetry, the more words predicted the more likely it will end up gibberish. Why?<br />Because you are more likely to hit words not in the training set<br />It doesn&rsquo;t, the likelihood of gibberish doesn&rsquo;t change<br />Because the probability of prediction compounds, and thus increases overall<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Because the probability that each word matches an existing phrase goes down the more words you create</span><span style="font:14px HelveticaNeue; color:#262626;"><br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">7.Question 7<br /></span><span style="font:14px HelveticaNeue; color:#262626;">What is a major drawback of word-based training for text generation instead of character-based generation?<br />There is no major drawback, it&rsquo;s always better to do word-based training<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Because there are far more words in a typical corpus than characters, it is much more memory intensive</span><span style="font:14px HelveticaNeue; color:#262626;"><br />Character based generation is more accurate because there are less characters to predict<br />Word based generation is more accurate because there is a larger body of words to draw from<br />1 point<br /></span><span style="font:21px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">8.Question 8<br /></span><span style="font:14px HelveticaNeue; color:#262626;">How does an LSTM help understand meaning when words that qualify each other aren&rsquo;t necessarily beside each other in a sentence?<br />They load all words into a cell state<br /></span><span style="font:14px HelveticaNeue-Bold; font-weight:bold; color:#262626;font-weight:bold; ">Values from earlier words can be carried to later ones via a cell state</span><span style="font:14px HelveticaNeue; color:#262626;"><br />They don&rsquo;t<br />They shuffle the words randomly<br />1 point</span><p class="blog-entry-tags">Tags: <a href="tag-tensorflow.html" title="TensorFlow" rel="tag">TensorFlow</a>, <a href="tag-nlp.html" title="NLP" rel="tag">NLP</a>, <a href="tag-deeplearning.html" title="DeepLearning" rel="tag">DeepLearning</a></p><div class="blog-entry-comments"><div id="disqus_thread">
<script type="text/javascript"> var disqus_shortname = 'blog';
(function() {
	var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	var disqus_identifier = '0c7595d00bde409bb699bc21a3a98aab';
	dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
	(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>
</div></div></div></div>
	</div>
	

			</section>
			<aside id="sidebar" class="large-4 columns">
				<!-- Sidebar content -->
				<h4 id="sidebar_title"></h4>
				<div id="sidebar_content"><br /><form style="border:1px solid #ccc;padding:3px;text-align:center;" action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open('https://feedburner.google.com/fb/a/mailverify?uri=tingWoBB', 'popupwindow', 'scrollbars=yes,width=550,height=520');return true"><p>Enter your email address<br />Subscribe My Blog:</p><p><input type="text" style="width:140px" name="email"/></p><input type="hidden" value="tingWoBB" name="uri"/><input type="hidden" name="loc" value="en_US"/><input type="submit" value="Subscribe" /></form></div>
				<div id="archives">
					<div id="blog-categories"><a href="category-data-science.html" class="blog-category-link-enabled">Data Science (12)</a><br /><a href="category-mooc.html" class="blog-category-link-enabled">Mooc (10)</a><br /><a href="category-personal.html" class="blog-category-link-enabled">Personal (2)</a><br /><a href="category-random-thoughts.html" class="blog-category-link-enabled">Random Thoughts (1)</a><br /></div><ul class="blog-tag-cloud"><li><a href="tag-deeplearning.html" title="DeepLearning" class="blog-tag-size-10" rel="tag">DeepLearning</a></li>
<li><a href="tag-machine-learning.html" title="Machine Learning" class="blog-tag-size-8" rel="tag">Machine Learning</a></li>
<li><a href="tag-nlp.html" title="NLP" class="blog-tag-size-9" rel="tag">NLP</a></li>
<li><a href="tag-tensorflow.html" title="TensorFlow" class="blog-tag-size-11" rel="tag">TensorFlow</a></li>
</ul>
<div id="blog-rss-feeds"><a class="blog-rss-link" href="http://feeds.feedburner.com/tingWoBB" rel="alternate" type="application/rss+xml" title="Patrick's blog">RSS </a><br /></div>
				</div>
			</aside>
		</div>

		<!-- Footer -->
		<footer class="row site_width">
			<div id="footer_content" class="large-12 columns">
				<div id="breadcrumb_container">
					<i class="fa fa-folder-open-o"></i> <span id="breadcrumb"></span>
				</div>
				&copy; 2019 Pengyuan Li <a href="#" id="rw_email_contact">Contact Me</a><script type="text/javascript">var _rwObsfuscatedHref0 = "mai";var _rwObsfuscatedHref1 = "lto";var _rwObsfuscatedHref2 = ":li";var _rwObsfuscatedHref3 = "pat";var _rwObsfuscatedHref4 = "ric";var _rwObsfuscatedHref5 = "k66";var _rwObsfuscatedHref6 = "6@g";var _rwObsfuscatedHref7 = "mai";var _rwObsfuscatedHref8 = "l.c";var _rwObsfuscatedHref9 = "om";var _rwObsfuscatedHref = _rwObsfuscatedHref0+_rwObsfuscatedHref1+_rwObsfuscatedHref2+_rwObsfuscatedHref3+_rwObsfuscatedHref4+_rwObsfuscatedHref5+_rwObsfuscatedHref6+_rwObsfuscatedHref7+_rwObsfuscatedHref8+_rwObsfuscatedHref9; document.getElementById("rw_email_contact").href = _rwObsfuscatedHref;</script>
			</div>
		</footer>

		<!-- Scroll up button -->
		<div id="scroll_up_button"><i class="fa fa-angle-up"></i></div>

		<!-- Handles loading Skrollr, which helps in animating portions of the header area. -->
		<!-- We check to see if the user is on an mobile device or not, and only serve up -->
		<!-- the animations on non-mobile devices. -->
		<script>
			$elixir(window).load(function() {
			  if(!(/Android|iPhone|iPad|iPod|BlackBerry|Windows Phone/i).test(navigator.userAgent || navigator.vendor || window.opera)){
			      skrollr.init({
			          forceHeight: false
			      });
			  }
			});
		</script>

	</body>

</html>
